C:\Users\Aavivi\PycharmProjects\HWSWCo-DesignProj\venv\Scripts\python.exe C:\Users\Aavivi\PycharmProjects\HWSWCo-DesignProj\parallel_cachine_encoder.py 
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[EMPTY]: Mean +- std dev: 2.49 ms +- 0.18 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[EMPTY]: Mean +- std dev: 690 ms +- 41 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[EMPTY]: Mean +- std dev: 705 ms +- 53 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[SIMPLE]: Mean +- std dev: 2.24 ms +- 0.06 ms
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (40.6 ms) is 11% of the mean (361 ms)
* the maximum (574 ms) is 59% greater than the mean (361 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[SIMPLE]: Mean +- std dev: 361 ms +- 41 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[SIMPLE]: Mean +- std dev: 343 ms +- 20 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[NESTED]: Mean +- std dev: 5.66 ms +- 0.18 ms
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (47.0 ms) is 12% of the mean (383 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[NESTED]: Mean +- std dev: 383 ms +- 47 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[NESTED]: Mean +- std dev: 349 ms +- 25 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)
* the maximum (9.55 ms) is 53% greater than the mean (6.23 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[HUGE]: Mean +- std dev: 6.23 ms +- 0.47 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[HUGE]: Mean +- std dev: 1.04 ms +- 0.08 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[HUGE]: Mean +- std dev: 666 us +- 53 us
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[GIANT]: Mean +- std dev: 63.1 ms +- 2.8 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[GIANT]: Mean +- std dev: 5.97 ms +- 0.46 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[GIANT]: Mean +- std dev: 679 us +- 59 us
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (17.1 ms) is 24% of the mean (70.6 ms)
* the maximum (143 ms) is 102% greater than the mean (70.6 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[GIANT_REPEAT]: Mean +- std dev: 70.6 ms +- 17.1 ms
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (1.45 ms) is 13% of the mean (11.2 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[GIANT_REPEAT]: Mean +- std dev: 11.2 ms +- 1.5 ms
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (496 us) is 12% of the mean (4.29 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[GIANT_REPEAT]: Mean +- std dev: 4.29 ms +- 0.50 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[WIDE_DUP]: Mean +- std dev: 8.89 ms +- 0.41 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[WIDE_DUP]: Mean +- std dev: 5.42 ms +- 0.23 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[WIDE_DUP]: Mean +- std dev: 583 ms +- 52 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[WIDE_UNQ]: Mean +- std dev: 3.46 ms +- 0.11 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[WIDE_UNQ]: Mean +- std dev: 10.1 ms +- 0.4 ms
.....................
WARNING: the benchmark result may be unstable
* Not enough samples to get a stable result (95% certainly of less than 1% variation)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[WIDE_UNQ]: Mean +- std dev: 557 ms +- 26 ms
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (1.22 sec) is 16% of the mean (7.59 sec)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps[DEEP]: Mean +- std dev: 7.59 sec +- 1.22 sec
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (95.7 ms) is 28% of the mean (344 ms)
* the maximum (725 ms) is 111% greater than the mean (344 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache[DEEP]: Mean +- std dev: 344 ms +- 96 ms
.....................
WARNING: the benchmark result may be unstable
* the standard deviation (4.92 ms) is 22% of the mean (22.5 ms)
* the maximum (41.5 ms) is 85% greater than the mean (22.5 ms)

Try to rerun the benchmark with more runs, values and/or loops.
Run 'python.exe -m pyperf system tune' command to reduce the system jitter.
Use pyperf stats, pyperf dump and pyperf hist to analyze results.
Use --quiet option to hide these warnings.

json_dumps_cache_par[DEEP]: Mean +- std dev: 22.5 ms +- 4.9 ms

Process finished with exit code 0
